{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from reward_model import FormatReward, AnswerReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 4096 # Can increase for longer reasoning traces\n",
    "lora_rank = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 20:52:15 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.676 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 72.1%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.68 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 10.94 GB. Also swap space = 5 GB.\n",
      "INFO 04-07 20:52:24 config.py:549] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-07 20:52:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":224}, use_cached_outputs=False, \n",
      "INFO 04-07 20:52:26 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-07 20:52:31 model_runner.py:1110] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
      "INFO 04-07 20:52:31 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W407 20:52:31.595028747 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 20:52:32 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0395f65b00444dddbb4c984db221a101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390d352144c94749a4d344593ef38383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 20:52:35 model_runner.py:1115] Loading model weights took 6.7252 GB\n",
      "INFO 04-07 20:52:35 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-07 20:52:38 worker.py:267] Memory profiling takes 2.24 seconds\n",
      "INFO 04-07 20:52:38 worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.72) = 17.07GiB\n",
      "INFO 04-07 20:52:38 worker.py:267] model weights take 6.73GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.25GiB; the rest of the memory reserved for KV Cache is 9.05GiB.\n",
      "INFO 04-07 20:52:38 executor_base.py:111] # cuda blocks: 10588, # CPU blocks: 5851\n",
      "INFO 04-07 20:52:38 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 41.36x\n",
      "INFO 04-07 20:52:40 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:15<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-07 20:52:56 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.67 GiB\n",
      "INFO 04-07 20:52:56 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 20.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.14 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.75, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2542fccb4a4128ba358a728721768c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Your task is to solve the following Sudoku puzzle.\n",
    "Empty cells are represented by blank spaces. Fill in all missing numbers such that each row, column, and 3Ã—3 subgrid contains the digits 1 through 9 exactly once.\n",
    "Put the fully solved Sudoku grid in the same format as the input, including dividers in answer section.\n",
    "Please respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def get_sudoku_probelms() -> Dataset:\n",
    "    data = load_dataset(\"csv\", data_files=\"problems/train.csv\")[\"train\"] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x[\"pretty_puzzle\"]}\n",
    "        ]\n",
    "    }) # type: ignore\n",
    "    return data\n",
    "\n",
    "dataset = get_sudoku_probelms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 8\n"
     ]
    }
   ],
   "source": [
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 3e-5,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 8, # Decrease if out of memory\n",
    "    max_prompt_length = 1000,\n",
    "    max_completion_length = 1000,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 50,\n",
    "    save_steps = 50,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,000 | Num Epochs = 1 | Total steps = 50\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 161,480,704/7,000,000,000 (2.31% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 41:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / tag_reward_func</th>\n",
       "      <th>rewards / tag_order_reward_func</th>\n",
       "      <th>rewards / answer_format_reward_func</th>\n",
       "      <th>rewards / correct_answer_reward_func</th>\n",
       "      <th>rewards / clue_preservation_reward_func</th>\n",
       "      <th>rewards / rule_reward_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.091435</td>\n",
       "      <td>1.138640</td>\n",
       "      <td>841.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.901200</td>\n",
       "      <td>1.279021</td>\n",
       "      <td>821.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106481</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609778</td>\n",
       "      <td>1.404555</td>\n",
       "      <td>865.875000</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.618266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192901</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.118722</td>\n",
       "      <td>1.190576</td>\n",
       "      <td>939.250000</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081790</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333930</td>\n",
       "      <td>1.695771</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.537247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200617</td>\n",
       "      <td>0.064815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.606482</td>\n",
       "      <td>1.198932</td>\n",
       "      <td>908.750000</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.543981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.881383</td>\n",
       "      <td>1.867186</td>\n",
       "      <td>870.250000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.583544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.647166</td>\n",
       "      <td>1.599199</td>\n",
       "      <td>905.750000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.617845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182099</td>\n",
       "      <td>0.097222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>3.708859</td>\n",
       "      <td>1.311353</td>\n",
       "      <td>682.125000</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.379630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.700793</td>\n",
       "      <td>1.312584</td>\n",
       "      <td>842.875000</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.541456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.796823</td>\n",
       "      <td>1.004835</td>\n",
       "      <td>602.875000</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.064815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.986111</td>\n",
       "      <td>0.398379</td>\n",
       "      <td>635.250000</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087963</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.260803</td>\n",
       "      <td>0.599513</td>\n",
       "      <td>656.125000</td>\n",
       "      <td>0.008491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.976852</td>\n",
       "      <td>0.675151</td>\n",
       "      <td>590.875000</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.787037</td>\n",
       "      <td>0.273148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>4.038580</td>\n",
       "      <td>0.211935</td>\n",
       "      <td>509.875000</td>\n",
       "      <td>0.007859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>3.202546</td>\n",
       "      <td>1.281419</td>\n",
       "      <td>655.500000</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.787037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.226852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>3.194444</td>\n",
       "      <td>0.697870</td>\n",
       "      <td>636.375000</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.689815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.143519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>3.805941</td>\n",
       "      <td>1.308483</td>\n",
       "      <td>652.750000</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.342593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3.246914</td>\n",
       "      <td>0.626179</td>\n",
       "      <td>515.375000</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233025</td>\n",
       "      <td>0.078704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.952546</td>\n",
       "      <td>1.122059</td>\n",
       "      <td>566.375000</td>\n",
       "      <td>0.013872</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.669753</td>\n",
       "      <td>0.631606</td>\n",
       "      <td>613.375000</td>\n",
       "      <td>0.014082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489198</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>3.804012</td>\n",
       "      <td>0.695235</td>\n",
       "      <td>585.875000</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.554012</td>\n",
       "      <td>0.365741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>3.774902</td>\n",
       "      <td>0.529139</td>\n",
       "      <td>570.625000</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.645062</td>\n",
       "      <td>0.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>4.043210</td>\n",
       "      <td>0.460327</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>0.013694</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.719136</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>4.379770</td>\n",
       "      <td>0.306796</td>\n",
       "      <td>516.375000</td>\n",
       "      <td>0.017955</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910494</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3.075372</td>\n",
       "      <td>0.858934</td>\n",
       "      <td>543.500000</td>\n",
       "      <td>0.025309</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.862795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280864</td>\n",
       "      <td>0.087963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>3.349291</td>\n",
       "      <td>1.368101</td>\n",
       "      <td>595.250000</td>\n",
       "      <td>0.016258</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501543</td>\n",
       "      <td>0.268519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>3.791667</td>\n",
       "      <td>0.495123</td>\n",
       "      <td>553.375000</td>\n",
       "      <td>0.027168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>0.108031</td>\n",
       "      <td>478.625000</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884259</td>\n",
       "      <td>0.504630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>3.935185</td>\n",
       "      <td>0.662333</td>\n",
       "      <td>428.125000</td>\n",
       "      <td>0.041102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.379630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>4.172840</td>\n",
       "      <td>0.252667</td>\n",
       "      <td>419.625000</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816358</td>\n",
       "      <td>0.439815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>3.881313</td>\n",
       "      <td>0.525073</td>\n",
       "      <td>512.250000</td>\n",
       "      <td>0.016651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.273148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>3.412178</td>\n",
       "      <td>0.654797</td>\n",
       "      <td>394.375000</td>\n",
       "      <td>0.033331</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.756313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452161</td>\n",
       "      <td>0.203704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3.101852</td>\n",
       "      <td>0.707599</td>\n",
       "      <td>500.625000</td>\n",
       "      <td>0.025910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.106481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>4.390432</td>\n",
       "      <td>0.166936</td>\n",
       "      <td>323.750000</td>\n",
       "      <td>0.040863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899691</td>\n",
       "      <td>0.532407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>4.035494</td>\n",
       "      <td>0.143434</td>\n",
       "      <td>426.500000</td>\n",
       "      <td>0.037836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762346</td>\n",
       "      <td>0.314815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>4.151235</td>\n",
       "      <td>0.138541</td>\n",
       "      <td>383.125000</td>\n",
       "      <td>0.027619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.794753</td>\n",
       "      <td>0.398148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>4.328704</td>\n",
       "      <td>0.114477</td>\n",
       "      <td>440.500000</td>\n",
       "      <td>0.026702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.537037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>4.034792</td>\n",
       "      <td>0.572325</td>\n",
       "      <td>414.375000</td>\n",
       "      <td>0.028066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770062</td>\n",
       "      <td>0.310185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>4.055555</td>\n",
       "      <td>0.578198</td>\n",
       "      <td>404.500000</td>\n",
       "      <td>0.027636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689815</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>4.083333</td>\n",
       "      <td>0.465256</td>\n",
       "      <td>384.250000</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.356481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>2.847678</td>\n",
       "      <td>1.683670</td>\n",
       "      <td>639.750000</td>\n",
       "      <td>0.028701</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.713805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507716</td>\n",
       "      <td>0.157407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>4.013889</td>\n",
       "      <td>0.441665</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>0.034413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.273148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>4.328704</td>\n",
       "      <td>0.194077</td>\n",
       "      <td>488.750000</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.856481</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>3.905864</td>\n",
       "      <td>0.618937</td>\n",
       "      <td>433.500000</td>\n",
       "      <td>0.028740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614197</td>\n",
       "      <td>0.347222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3.881453</td>\n",
       "      <td>0.598567</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.878367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706790</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>4.208333</td>\n",
       "      <td>0.149674</td>\n",
       "      <td>470.500000</td>\n",
       "      <td>0.021608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>3.475519</td>\n",
       "      <td>1.338138</td>\n",
       "      <td>507.875000</td>\n",
       "      <td>0.021723</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.896044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591049</td>\n",
       "      <td>0.300926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>2.819444</td>\n",
       "      <td>0.197414</td>\n",
       "      <td>429.500000</td>\n",
       "      <td>0.028618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>4.200617</td>\n",
       "      <td>0.097601</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>0.021079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.890432</td>\n",
       "      <td>0.310185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.0006948188267504562, metrics={'train_runtime': 2522.9299, 'train_samples_per_second': 0.159, 'train_steps_per_second': 0.02, 'total_flos': 0.0, 'train_loss': 0.0006948188267504562})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        FormatReward.tag_reward_func,\n",
    "        FormatReward.tag_order_reward_func,\n",
    "        AnswerReward.answer_format_reward_func,\n",
    "        AnswerReward.correct_answer_reward_func,\n",
    "        AnswerReward.clue_preservation_reward_func,\n",
    "        AnswerReward.rule_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.22s/it, est. speed input: 51.00 toks/s, output: 59.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "1. Start by filling in the first row. Since 2 and 5 are already in the first row, the missing number is 3.\n",
      "2. Continue this process for the rest of the grid, filling in numbers based on the given numbers and the rules of Sudoku.\n",
      "3. Use the process of elimination and checking the possibilities for each cell to solve the puzzle.\n",
      "</reasoning>\n",
      "\n",
      "<answer>\n",
      "  2 5 3 | 7 4 9 | 1 6 8  \n",
      "  7 4 8 | 5 6 3 | 9 8 2  \n",
      "  3 1 6 | 8 1 2 | 7 5 4  \n",
      "- - - + - - - + - - -  \n",
      "  1 9 7 | 8 5 4 | 6 3 2  \n",
      "  5 8 2 | 4 3 6 | 3 1 8  \n",
      "  6 4 3 | 2 7 1 | 5 9 7  \n",
      "- - - + - - - + - - -  \n",
      "  9 8 4 | 1 7 5 | 3 2 6  \n",
      "  6 2 5 | 3 8 7 | 4 4 9  \n",
      "  1 3 9 | 2 6 8 | 7 1 5  \n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "content = dataset[2][\"pretty_puzzle\"]\n",
    "\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : content},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2 5 |   4   | 1    \n",
      "  7 4 | 5 6 3 | 9 8 2\n",
      "3   6 | 8 1 2 |   5 4\n",
      "- - - + - - - + - - -\n",
      "      |   8   | 6    \n",
      "  5   | 4   6 | 2 1 8\n",
      "6 4   | 2   1 | 5   7\n",
      "- - - + - - - + - - -\n",
      "9 8   |       |   2  \n",
      "  6 2 | 3     |   9  \n",
      "  1 3 |   2 9 | 8 7  \n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
